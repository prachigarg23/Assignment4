{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2af245",
   "metadata": {},
   "source": [
    "## Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac3bbe",
   "metadata": {},
   "source": [
    "### Sub-aperture views \n",
    "<img align=\"center\" src=\"data/results/q12.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28160924",
   "metadata": {},
   "source": [
    "Key idea - focusing as taking multiple pinhole images and shift such that in all of them, the thing that is at the depth i want to focus is aligned. If I shift like this and sum them up, then I will get a lens which is in focus at the depth. \n",
    "\n",
    "The reason why if we shift by something proportional to depth, then everything at that depth will be aligned is because of disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e4634",
   "metadata": {},
   "source": [
    "### Refocusing and focal-stack simulation\n",
    "\n",
    "The interpolation is on s and t. The shift is a function of the scaled u, v and d. We are interpolating to get the shifted sub-aperture (pinhole views) images from each pinhole view (u, v coordinates). \n",
    "\n",
    "d = 0.0 \n",
    "<img align=\"center\" src=\"data/results/refocus_d0.png\" width=\"400\">\n",
    "d = 0.3\n",
    "<img align=\"center\" src=\"data/results/refocus_d1e-3.png\" width=\"400\">\n",
    "d = 0.5\n",
    "<img align=\"center\" src=\"data/results/refocus_d1e-5.png\" width=\"400\">\n",
    "d = 1.0\n",
    "<img align=\"center\" src=\"data/results/refocus_d1.png\" width=\"400\">\n",
    "d = 1.5\n",
    "<img align=\"center\" src=\"data/results/refocus_d15e-1.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Detail - shifted in v had to be flipped (it is responsible for iterating over rows (its reversed in interpolate function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c7a8d2",
   "metadata": {},
   "source": [
    "### All-in-focus image and depth from focus\n",
    "self note. The XYZ color space is the original model developed by the CIE. The Y channel represents the luminance of a color. The Z channel approximately relates to the amount of blue in an image, but the value of Z in the XYZ color space is not identical to the value of B in the RGB color space. The X channel does not have a clear color analogy. However, if you consider the XYZ color space as a 3-D coordinate system, then X values lie along the axis that is orthogonal to the Y (luminance) axis and the Z axis.\n",
    "\n",
    "\n",
    "#### Inferences and insights: \n",
    "\n",
    "$\\sigma_1$ is used to extract the high frequency details of the texture from the image. $\\sigma_2$ on the other hand is used to spread out the in-focus sharpness weights to regions outside of the edges so those regions also get weights (they will likely be skipped as they lack clear texture/detail). \n",
    "\n",
    "$\\sigma_1 = 4, \\sigma_2 = 8$\n",
    "<img align=\"center\" src=\"data/results/wsharpness_d1e-3_48.png\" width=\"900\">\n",
    "\n",
    "$\\sigma_1 = 6, \\sigma_2 = 4$, notice less of the spread of sharpness weights to neighbouhood with lower $\\sigma_2$\n",
    "<img align=\"center\" src=\"data/results/wsharpness_d1e-3_64.png\" width=\"900\">\n",
    "\n",
    "$\\sigma_1 = 6, \\sigma_2 = 8$, if both are high, then detail from unfocused regions also comes into the sharpness weights. \n",
    "<img align=\"center\" src=\"data/results/wsharpness_d1e-3_68.png\" width=\"900\">\n",
    "\n",
    "All in focus images and corrsponding Depth maps :\n",
    "\n",
    "<img align=\"center\" src=\"data/results/allfocus_4_8.png\" width=\"900\">\n",
    "\n",
    "<img align=\"center\" src=\"data/results/allfocus_6_4.png\" width=\"900\">\n",
    "\n",
    "<img align=\"center\" src=\"data/results/allfocus_6_8.png\" width=\"900\">\n",
    "\n",
    "<img align=\"center\" src=\"data/results/allfocus_4_4.png\" width=\"900\">\n",
    "\n",
    "<br> \n",
    "The parts at the top of the image on the black chess pieces have unclear depth estimation. This is likely because depth estimation depends only on sharpness weight maps and depth. Visualising the sharpness maps shows that the black pieces are not clearly captured and remain indistinguishable. \n",
    "\n",
    "<br> On the other hand, the all in focus image is computed by applying the sharpness weight maps on the refocused images at each depth (which clearly captures the black chess pieces properly at the depth in focus). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e720e0",
   "metadata": {},
   "source": [
    "### Focal-aperture stack and confocal stereo\n",
    "\n",
    "Focal aperture stack:\n",
    "<img align=\"center\" src=\"data/results/focal_aperture_stack.png\" width=\"900\">\n",
    "<img align=\"center\" src=\"data/results/difference.png\" width=\"900\">\n",
    "\n",
    "AFI: pixel (200, 200)\n",
    "<img align=\"center\" src=\"data/results/AFI_pix1_200200.png\" width=\"800\">\n",
    "\n",
    "AFI: pixel (255, 500)\n",
    "<img align=\"center\" src=\"data/results/AFI_pix2_255_500.png\" width=\"800\">\n",
    "\n",
    "AFI: pixel (312, 400)\n",
    "<img align=\"center\" src=\"data/results/AFI_pix2_312_400.png\" width=\"800\">\n",
    "\n",
    "I selected depth for each channel based on their respective intensity variation across apertures. \n",
    "<img align=\"center\" src=\"data/results/separate_colors.png\" width=\"600\">\n",
    "I selected depth based on 1st channel's intensity variation across apertures. \n",
    "<img align=\"center\" src=\"data/results/same_colors.png\" width=\"600\">\n",
    "\n",
    "The depth map is not smooth and very pixelated as we are computing depth for each pixel. But it is more accurate and provides more information as it is not dependent on the size of the 2 blur kernels in depth from focus case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d1013",
   "metadata": {},
   "source": [
    "### Capture and refocus your own lightfield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dafd0f",
   "metadata": {},
   "source": [
    "VIDEO SAMPLE FRAMES \n",
    "<img align=\"center\" src=\"data/results/q3.png\" width=\"800\">\n",
    "\n",
    "Template image with template and search space \n",
    "<img align=\"center\" src=\"data/results/patch.png\" width=\"800\">\n",
    "Template \n",
    "<img align=\"center\" src=\"data/results/template.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9224b41",
   "metadata": {},
   "source": [
    "Assignment Discussion Credits - Sriram Narayanan, Neeraj Panse, Aman Mehra "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp photo",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
